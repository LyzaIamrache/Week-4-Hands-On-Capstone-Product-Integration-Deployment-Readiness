{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a10fbb4",
      "metadata": {
        "id": "2a10fbb4"
      },
      "source": [
        "# CS 5588 — Data Science Capstone\n",
        "## Week 4 Hands-On — Capstone Product Integration Sprint  \n",
        "**Deadline:** Feb. 12, 2026 (Thu), Midnight\n",
        "\n",
        "### What this week is about\n",
        "This Week 4 hands-on upgrades your Week 3 prototype into a **capstone-ready product module**:\n",
        "- **Application integration** (a simple UI or endpoint that demonstrates a real workflow)\n",
        "- **Operational logging + monitoring** (so you can measure usage and failures)\n",
        "- **Impact-focused evaluation** (not only IR metrics — also stakeholder/product impact)\n",
        "- **Deployment readiness plan** (architecture + run instructions)\n",
        "- **Failure & risk analysis** (what can go wrong, how you detect/mitigate)\n",
        "\n",
        "### Submission policy\n",
        "- **Team deliverables (GitHub):** code + notebook + brief/report + screenshots/diagram  \n",
        "- **Individual reflection (Canvas/Survey):** one short paragraph\n",
        "\n",
        "> This notebook is a **template**. Replace placeholders with your project specifics (data, users, goals, models).\n",
        "\n",
        "---\n",
        "\n",
        "## Recommended repo structure\n",
        "```\n",
        "/app/                 # Streamlit (or other) UI (recommended)\n",
        "/src/                 # reusable pipeline code (data + modeling + retrieval)\n",
        "/logs/                # monitoring logs (auto-created)\n",
        "/reports/             # integration brief + diagrams\n",
        "/notebooks/           # this notebook\n",
        "requirements.txt\n",
        "README.md\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Checklist (quick)\n",
        "- [ ] Week-4 Integration Brief completed (Section 2)\n",
        "- [ ] Working app demo (Section 6)\n",
        "- [ ] Logging file created and populated (Section 4–5)\n",
        "- [ ] Impact evaluation + technical metrics (Section 5)\n",
        "- [ ] Deployment plan + architecture diagram (Section 7)\n",
        "- [ ] One realistic failure/risk + mitigation (Section 8)\n",
        "- [ ] Individual reflection (Section 9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf50b58b",
      "metadata": {
        "id": "bf50b58b"
      },
      "source": [
        "## 1) Team & project metadata (Required)\n",
        "Fill these fields first. They will be reused in your report and README."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJjhB5ypiceO"
      },
      "source": [
        "## ✅ Step 0.5 — Fill in your project info (required)\n",
        "\n",
        "Before generating files, update the configuration values below. This prevents generic submissions.\n"
      ],
      "id": "tJjhB5ypiceO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiTbpVfoiceO",
        "outputId": "85ad6b91-3418-4c87-98b7-28578da7adc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Week4Config(team_name='HappyGroup', project_title='SmartCampus: Trust-Aware Multimodal GenAI Digital Twin for UMKC', stakeholder='UMKC students, visitors, and campus staff', app_dir='./app', src_dir='./src', logs_dir='./logs', reports_dir='./reports', log_file='./logs/product_metrics.csv')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip -q install pymupdf streamlit\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "@dataclass\n",
        "class Week4Config:\n",
        "    team_name: str = \"HappyGroup\"\n",
        "    project_title: str = \"SmartCampus: Trust-Aware Multimodal GenAI Digital Twin for UMKC\"\n",
        "    stakeholder: str = \"UMKC students, visitors, and campus staff\"\n",
        "\n",
        "    app_dir: str = \"./app\"\n",
        "    src_dir: str = \"./src\"\n",
        "    logs_dir: str = \"./logs\"\n",
        "    reports_dir: str = \"./reports\"\n",
        "\n",
        "    log_file: str = \"./logs/product_metrics.csv\"   # REQUIRED path\n",
        "\n",
        "cfg = Week4Config()\n",
        "for d in [cfg.app_dir, cfg.src_dir, cfg.logs_dir, cfg.reports_dir]:\n",
        "    Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cfg\n"
      ],
      "id": "WiTbpVfoiceO"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b213d49c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b213d49c",
        "outputId": "8fd45fd5-5d12-4deb-fae0-540254f3dfb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                   doc_id   domain  page  \\\n",
              " 0              volker_map      map     1   \n",
              " 1              volker_map      map     2   \n",
              " 2              health_map      map     1   \n",
              " 3              health_map      map     2   \n",
              " 4  umkc_catalog_2025_2026  catalog     1   \n",
              " \n",
              "                                                 text  \n",
              " 0  Brush Creek\\nBrush Creek\\nKauffman\\nLegacy\\nLa...  \n",
              " 1  UMKC Volker campus buildings and addresses\\nAD...  \n",
              " 2  68M\\nReserved\\nDental\\nPatient\\nParking\\n67\\n2...  \n",
              " 3  UMKC Health Sciences campus buildings and addr...  \n",
              " 4  TABLE OF CONTENTS\\nHome: 2025-2026 Catalog ......  ,\n",
              " doc_id\n",
              " umkc_catalog_2025_2026        2131\n",
              " clery_report_2025              117\n",
              " visual_identity_guidelines      73\n",
              " student_parking_permits          5\n",
              " volker_map                       2\n",
              " health_map                       2\n",
              " spring_2026_shuttle              1\n",
              " Name: count, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "PDFS = [\n",
        "    (\"umkc-volker-campus-map.pdf\", \"volker_map\", \"map\"),\n",
        "    (\"umkc-health-sciences-campus-map.pdf\", \"health_map\", \"map\"),\n",
        "    (\"2025-2026 University Catalog_Archived 9-10-25.pdf\", \"umkc_catalog_2025_2026\", \"catalog\"),\n",
        "    (\"2025ccfsr.pdf\", \"clery_report_2025\", \"safety\"),\n",
        "    (\"2026-spring-shuttle-schedule.pdf\", \"spring_2026_shuttle\", \"shuttle\"),\n",
        "    (\"Student Permits - Parking Options - Parking _ University of Missouri-Kansas City.pdf\", \"student_parking_permits\", \"parking\"),\n",
        "    (\"visual-identity-guidelines.pdf\", \"visual_identity_guidelines\", \"brand\"),\n",
        "]\n",
        "\n",
        "def extract_pdf_pages(pdf_path: str):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = []\n",
        "    for i, page in enumerate(doc):\n",
        "        txt = page.get_text(\"text\")\n",
        "        pages.append({\"page\": i+1, \"text\": txt})\n",
        "    return pages\n",
        "\n",
        "docs = []\n",
        "for path, doc_id, domain in PDFS:\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        print(\"MISSING:\", path)\n",
        "        continue\n",
        "    for pg in extract_pdf_pages(str(p)):\n",
        "        docs.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"domain\": domain,\n",
        "            \"page\": pg[\"page\"],\n",
        "            \"text\": pg[\"text\"].strip()\n",
        "        })\n",
        "\n",
        "raw_df = pd.DataFrame(docs)\n",
        "raw_df.head(), raw_df[\"doc_id\"].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0868900",
      "metadata": {
        "id": "e0868900"
      },
      "source": [
        "## 2) Week-4 Capstone Integration Brief (Required)\n",
        "Create a **1-page** brief (can be in `reports/week4_integration_brief.md` or a README section).\n",
        "\n",
        "Include:\n",
        "1. **Where this module fits** in your capstone architecture  \n",
        "2. **Primary user workflow** (what the user does end-to-end)  \n",
        "3. **Success metrics** (product/impact metrics + technical metrics)  \n",
        "4. **Risks if it fails** (stakeholder harm / wrong decision / wasted time)  \n",
        "5. **Next sprint** (what you would build next)\n",
        "\n",
        "Below is a starter you can export to Markdown.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e3de8750",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "e3de8750",
        "outputId": "6c9b1f52-b62c-4240-fd81-1c26910a2b60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Week4Config' object has no attribute 'problem_statement'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3811532849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mTeam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteam_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mStakeholder\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mUser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstakeholder\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;34m**\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_statement\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Week4Config' object has no attribute 'problem_statement'"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "brief_path = Path(cfg.reports_dir) / \"week4_integration_brief.md\"\n",
        "\n",
        "brief_template = f\"\"\"# Week 4 Integration Brief — {cfg.project_title}\n",
        "**Team:** {cfg.team_name}\n",
        "**Stakeholder/User:** {cfg.stakeholder}\n",
        "**Problem:** {cfg.problem_statement}\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Module placement in capstone system\n",
        "\n",
        "**Upstream inputs:**\n",
        "- User query (text or multimodal input)\n",
        "- Campus knowledge base (policies, services, building/location data)\n",
        "- Retrieval configuration (vector / hybrid search)\n",
        "\n",
        "**Module responsibilities:**\n",
        "- Retrieve top-K relevant campus knowledge chunks\n",
        "- Generate grounded response using RAG\n",
        "- Display evidence IDs/snippets\n",
        "- Compute confidence / faithfulness indicator\n",
        "- Log metrics to logs/product_metrics.csv\n",
        "\n",
        "**Downstream outputs:**\n",
        "- Evidence-backed answer in UI\n",
        "- Confidence score\n",
        "- Structured query log for monitoring & governance\n",
        "\n",
        "---\n",
        "\n",
        "## 2) User workflow (end-to-end)\n",
        "\n",
        "1. User submits a campus-related question in the SmartCampus interface.\n",
        "2. System retrieves relevant verified UMKC documents (hybrid retrieval).\n",
        "3. LLM generates a grounded response using retrieved evidence.\n",
        "4. UI displays:\n",
        "   - Answer\n",
        "   - Evidence snippets + IDs\n",
        "   - Confidence/faithfulness signal\n",
        "5. Interaction is logged automatically for monitoring.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Success metrics\n",
        "\n",
        "### Product / impact metrics (required)\n",
        "\n",
        "- **Time-to-decision:**\n",
        "  Reduce campus information lookup from ~5–10 minutes manual search\n",
        "  to < 10 seconds automated retrieval.\n",
        "\n",
        "- **Trust/verification signals:**\n",
        "  ≥90% of answers include visible citations.\n",
        "  Low-confidence answers trigger warning/refusal behavior.\n",
        "\n",
        "- **Adoption/usage signal:**\n",
        "  Increasing weekly query count from students/staff.\n",
        "\n",
        "### Technical metrics (recommended)\n",
        "\n",
        "- **Quality:**\n",
        "  Precision@5 ≥ 0.75\n",
        "  Recall@10 ≥ 0.85\n",
        "  Citation coverage ≥ 0.90\n",
        "\n",
        "- **Latency:**\n",
        "  Target < 3 seconds per query (UI-visible response time).\n",
        "\n",
        "- **Failure rate:**\n",
        "  < 5% low-confidence or empty retrieval cases.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Failure & risk (what happens if wrong?)\n",
        "\n",
        "- **Likely failure:**\n",
        "  Retrieval mismatch or outdated campus data.\n",
        "\n",
        "- **Impact:**\n",
        "  Student receives incorrect office location or policy instruction,\n",
        "  leading to wasted time or wrong decision.\n",
        "\n",
        "- **Mitigation:**\n",
        "  - Confidence threshold with refusal behavior\n",
        "  - Visible evidence display\n",
        "  - Logging & monitoring of low-confidence events\n",
        "  - Regular knowledge base updates\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Next sprint plan\n",
        "\n",
        "- **Next feature:**\n",
        "  Multimodal extension (image-to-location recognition for campus navigation).\n",
        "\n",
        "- **Data improvement:**\n",
        "  Add structured campus service database + real-time updates.\n",
        "\n",
        "- **Evaluation improvement:**\n",
        "  Human-in-the-loop validation set for retrieval accuracy + calibration testing.\n",
        "\"\"\"\n",
        "\n",
        "brief_path.write_text(brief_template, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Wrote:\", brief_path)\n",
        "print(\"\\nPreview (first 25 lines):\\n\")\n",
        "print(\"\\n\".join(brief_template.splitlines()[:25]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41f8e298",
      "metadata": {
        "id": "41f8e298"
      },
      "source": [
        "## 3) Data + modeling hook (Project-aligned)\n",
        "Week 4 must use **your capstone project data and models**.\n",
        "\n",
        "- If you are building a **RAG / search / recommender**: wire your retrieval + generation here.\n",
        "- If you are building a **predictive model**: wire your training/inference function here.\n",
        "- If you are building a **dashboard / analytics product**: wire your data processing + visualization logic here.\n",
        "\n",
        "Below is a **minimal runnable stub** so this notebook executes even without your data.\n",
        "Replace the stubs with your actual project code from Week 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b3a8a11b",
      "metadata": {
        "id": "b3a8a11b"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "import time\n",
        "\n",
        "# =========================\n",
        "# 1) Load ALL uploaded PDFs\n",
        "# =========================\n",
        "def load_data():\n",
        "    pdfs = sorted(Path(\".\").glob(\"*.pdf\"))\n",
        "    if not pdfs:\n",
        "        raise FileNotFoundError(\"No PDFs found in the notebook folder.\")\n",
        "\n",
        "    docs = []\n",
        "\n",
        "    for p in pdfs:\n",
        "        doc = fitz.open(str(p))\n",
        "        full_text = []\n",
        "\n",
        "        for page in doc:\n",
        "            txt = page.get_text(\"text\").strip()\n",
        "            if txt:\n",
        "                full_text.append(txt)\n",
        "\n",
        "        docs.append({\n",
        "            \"doc_id\": p.stem.replace(\" \", \"_\"),\n",
        "            \"text\": \"\\n\".join(full_text),\n",
        "            \"source\": p.name\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(docs)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) Simple Retrieval\n",
        "# =========================\n",
        "def retrieve_evidence(data, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
        "\n",
        "    scores = data[\"text\"].str.lower().apply(\n",
        "        lambda t: sum(w in t for w in query.lower().split())\n",
        "    )\n",
        "\n",
        "    top = data.assign(score=scores).sort_values(\"score\", ascending=False).head(k)\n",
        "\n",
        "    evidence = []\n",
        "    for _, row in top.iterrows():\n",
        "        evidence.append({\n",
        "            \"evidence_id\": row[\"doc_id\"],\n",
        "            \"snippet\": row[\"text\"][:400],\n",
        "            \"score\": float(row[\"score\"]),\n",
        "            \"source\": row[\"source\"]\n",
        "        })\n",
        "\n",
        "    return evidence\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3) Grounded Answer\n",
        "# =========================\n",
        "def generate_answer(query: str, evidence: List[Dict[str, Any]]) -> str:\n",
        "\n",
        "    if not evidence or max(e[\"score\"] for e in evidence) <= 0:\n",
        "        return \"I’m not confident I can answer from the current evidence.\"\n",
        "\n",
        "    cites = \", \".join([f\"[{e['evidence_id']}]\" for e in evidence[:2]])\n",
        "    return f\"Based on the retrieved UMKC documents {cites}, here is a grounded response.\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) Confidence\n",
        "# =========================\n",
        "def compute_confidence(evidence: List[Dict[str, Any]]) -> float:\n",
        "    if not evidence:\n",
        "        return 0.0\n",
        "    return float(min(1.0, max(e[\"score\"] for e in evidence) / 5.0))\n",
        "\n",
        "\n",
        "def faithfulness_indicator(confidence: float, threshold: float = 0.65) -> str:\n",
        "    return \"ok\" if confidence >= threshold else \"low_confidence_refusal_recommended\"\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5) Main Pipeline (FIXED)\n",
        "# =========================\n",
        "def run_capstone_pipeline(query: str, k: int = 3) -> Dict[str, Any]:\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    data = load_data()\n",
        "    evidence = retrieve_evidence(data, query=query, k=k)\n",
        "    answer = generate_answer(query, evidence)\n",
        "\n",
        "    conf = compute_confidence(evidence)\n",
        "    faith = faithfulness_indicator(conf)\n",
        "\n",
        "    latency_ms = int((time.time() - t0) * 1000)\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"evidence\": evidence,\n",
        "        \"evidence_ids\": [e[\"evidence_id\"] for e in evidence],\n",
        "        \"latency_ms\": latency_ms,\n",
        "        \"confidence\": round(conf, 3),\n",
        "        \"faithfulness\": faith\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "657b873d",
      "metadata": {
        "id": "657b873d"
      },
      "source": [
        "## 4) Monitoring & logging (Required)\n",
        "You must implement **automatic logging** for every user interaction / query.\n",
        "\n",
        "Minimum columns (recommended):\n",
        "- timestamp\n",
        "- event_type (query / feedback / error)\n",
        "- user_task_type (what workflow this supports)\n",
        "- config (model/retrieval settings)\n",
        "- latency_ms\n",
        "- output_quality_signal (e.g., faithfulness pass/fail, confidence, error flag)\n",
        "- artifact_ids (evidence ids, record ids, etc.)\n",
        "\n",
        "This creates the foundation for **production-style monitoring** and **capstone evaluation**.\n",
        "\n",
        "\n",
        "**Mapping to the Week 4 handout terminology (1-to-1):**\n",
        "- `artifact_ids` → **evidence IDs**\n",
        "- `model_or_mode` → **retrieval configuration**\n",
        "- `quality_signal` → **confidence/faithfulness indicator**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ea76d1e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "ea76d1e4",
        "outputId": "53d634c1-bb84-4e30-c790-f3d521a5bdbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to: ./logs/product_metrics.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          timestamp user_task_type retrieval_configuration  \\\n",
              "0  2026-02-13T01:08:44.101245+00:00  student_query          keyword_search   \n",
              "\n",
              "   latency_ms        evidence_ids  confidence faithfulness_indicator  \n",
              "0        14.8  [\"doc_1\", \"doc_2\"]        0.82                     ok  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0b43d35-a939-4375-82ba-7f5f62312fd1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>user_task_type</th>\n",
              "      <th>retrieval_configuration</th>\n",
              "      <th>latency_ms</th>\n",
              "      <th>evidence_ids</th>\n",
              "      <th>confidence</th>\n",
              "      <th>faithfulness_indicator</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2026-02-13T01:08:44.101245+00:00</td>\n",
              "      <td>student_query</td>\n",
              "      <td>keyword_search</td>\n",
              "      <td>14.8</td>\n",
              "      <td>[\"doc_1\", \"doc_2\"]</td>\n",
              "      <td>0.82</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0b43d35-a939-4375-82ba-7f5f62312fd1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0b43d35-a939-4375-82ba-7f5f62312fd1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0b43d35-a939-4375-82ba-7f5f62312fd1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"2026-02-13T01:08:44.101245+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_task_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"student_query\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieval_configuration\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"keyword_search\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 14.8,\n        \"max\": 14.8,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          14.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evidence_ids\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[\\\"doc_1\\\", \\\"doc_2\\\"]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.82,\n        \"max\": 0.82,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.82\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness_indicator\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ok\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "# REQUIRED per rubric\n",
        "LOG_FILE = \"./logs/product_metrics.csv\"\n",
        "\n",
        "LOG_COLUMNS = [\n",
        "    \"timestamp\",\n",
        "    \"user_task_type\",\n",
        "    \"retrieval_configuration\",\n",
        "    \"latency_ms\",\n",
        "    \"evidence_ids\",\n",
        "    \"confidence\",\n",
        "    \"faithfulness_indicator\"\n",
        "]\n",
        "\n",
        "def ensure_csv(path: str, header: list):\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if not p.exists():\n",
        "        with open(p, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "\n",
        "ensure_csv(LOG_FILE, LOG_COLUMNS)\n",
        "print(\"Logging to:\", LOG_FILE)\n",
        "\n",
        "\n",
        "def log_interaction(user_task_type: str,\n",
        "                    retrieval_configuration: str,\n",
        "                    latency_ms: float,\n",
        "                    evidence_ids,\n",
        "                    confidence: float,\n",
        "                    faithfulness_indicator: str):\n",
        "\n",
        "    row = [\n",
        "        datetime.now(timezone.utc).isoformat(),\n",
        "        user_task_type,\n",
        "        retrieval_configuration,\n",
        "        round(float(latency_ms), 2),\n",
        "        json.dumps(list(evidence_ids), ensure_ascii=False),\n",
        "        round(float(confidence), 3),\n",
        "        faithfulness_indicator\n",
        "    ]\n",
        "\n",
        "    with open(LOG_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "\n",
        "# Demo entry\n",
        "log_interaction(\n",
        "    user_task_type=\"student_query\",\n",
        "    retrieval_configuration=\"keyword_search\",\n",
        "    latency_ms=14.8,\n",
        "    evidence_ids=[\"doc_1\", \"doc_2\"],\n",
        "    confidence=0.82,\n",
        "    faithfulness_indicator=\"ok\"\n",
        ")\n",
        "\n",
        "pd.read_csv(LOG_FILE).tail(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a30529",
      "metadata": {
        "id": "c6a30529"
      },
      "source": [
        "## 5) Week-4 evaluation: Impact + Technical metrics (Required)\n",
        "Capstone evaluation must include **impact-oriented metrics**, not only technical ones.\n",
        "\n",
        "### A) Impact metrics (required)\n",
        "Choose 2–3 that match your stakeholder workflow:\n",
        "- time-to-decision (before vs after)\n",
        "- trust/verification rate (e.g., citations shown, evidence opened)\n",
        "- task success rate (user can complete task)\n",
        "- adoption signal (weekly active usage in demo, number of queries run)\n",
        "\n",
        "### B) Technical metrics (recommended)\n",
        "Pick metrics that match your system type:\n",
        "- Classification/regression: accuracy/F1/AUC/MAE + calibration\n",
        "- Retrieval/RAG: Precision@K/Recall@K + citation coverage + refusal correctness\n",
        "- Forecasting: MAE/MAPE/CRPS, interval coverage, etc.\n",
        "\n",
        "Below is a small example that computes simple metrics from your demo data.\n",
        "Replace with your project-specific metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "81a05317",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81a05317",
        "outputId": "aecd4e25-7f70-40ce-e9d2-a96c0f5fe8c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'impact_metrics': {'time_to_decision_baseline_min': 7.0,\n",
              "  'time_to_decision_after_avg_sec': 0.01,\n",
              "  'estimated_time_saved_min': 7.0,\n",
              "  'estimated_time_saved_pct': 100.0,\n",
              "  'trust_verification_rate_citation_shown': 1.0,\n",
              "  'adoption_signal_queries_logged': 1,\n",
              "  'adoption_signal_unique_task_types': 1},\n",
              " 'technical_metrics': {'citation_coverage': 1.0,\n",
              "  'low_confidence_rate': 0.0,\n",
              "  'faithfulness_ok_rate': 1.0,\n",
              "  'latency_ms_p50': 14.8,\n",
              "  'latency_ms_p95': 14.8,\n",
              "  'confidence_mean': 0.82}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "LOG_PATH = \"./logs/product_metrics.csv\"\n",
        "\n",
        "def _parse_json_list(x):\n",
        "    try:\n",
        "        v = json.loads(x) if isinstance(x, str) else x\n",
        "        return v if isinstance(v, list) else []\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def compute_week4_metrics_from_logs(log_path=LOG_PATH, low_conf_threshold=0.65, baseline_minutes=7.0):\n",
        "    df = pd.read_csv(log_path)\n",
        "\n",
        "    # If empty, return safely\n",
        "    if len(df) == 0:\n",
        "        return {\"impact_metrics\": {}, \"technical_metrics\": {}, \"note\": \"No log rows found.\"}\n",
        "\n",
        "    # Evidence column (supports both schemas)\n",
        "    if \"evidence_ids\" in df.columns:\n",
        "        df[\"evidence_list\"] = df[\"evidence_ids\"].apply(_parse_json_list)\n",
        "    elif \"artifact_ids\" in df.columns:\n",
        "        df[\"evidence_list\"] = df[\"artifact_ids\"].apply(_parse_json_list)\n",
        "    else:\n",
        "        df[\"evidence_list\"] = [[] for _ in range(len(df))]\n",
        "\n",
        "    df[\"has_citation\"] = df[\"evidence_list\"].apply(lambda xs: len(xs) > 0)\n",
        "\n",
        "    # Numeric columns\n",
        "    df[\"latency_ms\"] = pd.to_numeric(df.get(\"latency_ms\", np.nan), errors=\"coerce\")\n",
        "    df[\"confidence\"] = pd.to_numeric(df.get(\"confidence\", 0.5), errors=\"coerce\").fillna(0.5)\n",
        "\n",
        "    # Faithfulness column (if present)\n",
        "    if \"faithfulness_indicator\" in df.columns:\n",
        "        faith_ok_rate = float((df[\"faithfulness_indicator\"] == \"ok\").mean())\n",
        "    else:\n",
        "        faith_ok_rate = np.nan\n",
        "\n",
        "    # -------- Impact Metrics (required) --------\n",
        "    after_seconds = float(df[\"latency_ms\"].mean() / 1000.0) if df[\"latency_ms\"].notna().any() else np.nan\n",
        "    after_minutes = after_seconds / 60.0 if not np.isnan(after_seconds) else np.nan\n",
        "    time_saved_minutes = baseline_minutes - after_minutes if not np.isnan(after_minutes) else np.nan\n",
        "    time_saved_pct = (time_saved_minutes / baseline_minutes) * 100 if not np.isnan(time_saved_minutes) else np.nan\n",
        "\n",
        "    impact_metrics = {\n",
        "        \"time_to_decision_baseline_min\": baseline_minutes,\n",
        "        \"time_to_decision_after_avg_sec\": round(after_seconds, 2) if not np.isnan(after_seconds) else None,\n",
        "        \"estimated_time_saved_min\": round(time_saved_minutes, 2) if not np.isnan(time_saved_minutes) else None,\n",
        "        \"estimated_time_saved_pct\": round(time_saved_pct, 1) if not np.isnan(time_saved_pct) else None,\n",
        "        \"trust_verification_rate_citation_shown\": round(float(df[\"has_citation\"].mean()), 3),\n",
        "        \"adoption_signal_queries_logged\": int(len(df)),\n",
        "        \"adoption_signal_unique_task_types\": int(df[\"user_task_type\"].nunique()) if \"user_task_type\" in df.columns else None,\n",
        "    }\n",
        "\n",
        "    # -------- Technical Metrics (recommended) --------\n",
        "    citation_coverage = float(df[\"has_citation\"].mean())\n",
        "    low_conf_rate = float((df[\"confidence\"] < low_conf_threshold).mean())\n",
        "\n",
        "    latency_p50 = float(df[\"latency_ms\"].median()) if df[\"latency_ms\"].notna().any() else np.nan\n",
        "    latency_p95 = float(df[\"latency_ms\"].quantile(0.95)) if df[\"latency_ms\"].notna().any() else np.nan\n",
        "\n",
        "    technical_metrics = {\n",
        "        \"citation_coverage\": round(citation_coverage, 3),\n",
        "        \"low_confidence_rate\": round(low_conf_rate, 3),\n",
        "        \"faithfulness_ok_rate\": round(faith_ok_rate, 3) if not np.isnan(faith_ok_rate) else None,\n",
        "        \"latency_ms_p50\": round(latency_p50, 1) if not np.isnan(latency_p50) else None,\n",
        "        \"latency_ms_p95\": round(latency_p95, 1) if not np.isnan(latency_p95) else None,\n",
        "        \"confidence_mean\": round(float(df[\"confidence\"].mean()), 3),\n",
        "    }\n",
        "\n",
        "    return {\"impact_metrics\": impact_metrics, \"technical_metrics\": technical_metrics}\n",
        "\n",
        "# Run it\n",
        "compute_week4_metrics_from_logs(LOG_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb913dc1",
      "metadata": {
        "id": "fb913dc1"
      },
      "source": [
        "## 6) Build your capstone demo app (Required)\n",
        "Your team must expose the module via an application interface:\n",
        "- Streamlit UI (recommended), OR\n",
        "- an API endpoint + simple client, OR\n",
        "- a dashboard component integrated into your project\n",
        "\n",
        "### Required app behavior\n",
        "- Accept user input (question / task / parameters)\n",
        "- Produce output aligned to your project workflow\n",
        "- Display artifacts (evidence / records / plots) as appropriate\n",
        "- Log events automatically to `logs/week4_events.csv`\n",
        "\n",
        "Below is a Streamlit skeleton generator that you can commit to `/app/main.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2fc315c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fc315c0",
        "outputId": "45f05901-b8c8-49aa-bc1f-cba719ea5a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Streamlit app: app/main.py\n",
            "\n",
            "Run locally:\n",
            "  pip install streamlit pandas pymupdf\n",
            "  streamlit run app/main.py\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "streamlit_app = r'''\n",
        "import json, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "# Import your real pipeline (recommended)\n",
        "# Put your PDF-based pipeline into src/pipeline.py with run_capstone_pipeline(query, k)\n",
        "from src.pipeline import run_capstone_pipeline\n",
        "\n",
        "LOG_FILE = \"logs/week4_events.csv\"\n",
        "LOG_COLUMNS = [\n",
        "    \"timestamp\",\n",
        "    \"event_type\",\n",
        "    \"user_task_type\",\n",
        "    \"model_or_mode\",\n",
        "    \"latency_ms\",\n",
        "    \"artifact_ids\",\n",
        "    \"quality_signal\",\n",
        "    \"notes\"\n",
        "]\n",
        "\n",
        "def ensure_csv(path: str, header):\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if not p.exists():\n",
        "        pd.DataFrame(columns=header).to_csv(p, index=False)\n",
        "\n",
        "def log_event(event_type, user_task_type, model_or_mode, latency_ms, artifact_ids, quality_signal, notes=\"\"):\n",
        "    ensure_csv(LOG_FILE, LOG_COLUMNS)\n",
        "    row = {\n",
        "        \"timestamp\": pd.Timestamp.utcnow().isoformat(),\n",
        "        \"event_type\": event_type,\n",
        "        \"user_task_type\": user_task_type,\n",
        "        \"model_or_mode\": model_or_mode,\n",
        "        \"latency_ms\": round(float(latency_ms), 2),\n",
        "        \"artifact_ids\": json.dumps(list(artifact_ids), ensure_ascii=False),\n",
        "        \"quality_signal\": quality_signal,\n",
        "        \"notes\": notes\n",
        "    }\n",
        "    # append without re-reading full csv\n",
        "    df = pd.DataFrame([row])\n",
        "    df.to_csv(LOG_FILE, mode=\"a\", header=False, index=False)\n",
        "\n",
        "# --- UI ---\n",
        "st.set_page_config(page_title=\"SmartCampus — Week 4 Demo\", layout=\"wide\")\n",
        "st.title(\"SmartCampus — Week 4 Capstone Demo\")\n",
        "st.caption(\"Query input • Evidence display • Response panel • Metrics panel • Auto-logging (week4_events.csv)\")\n",
        "\n",
        "col1, col2, col3 = st.columns([1, 1, 2])\n",
        "with col1:\n",
        "    user_task_type = st.selectbox(\"User task type\", [\"student\", \"visitor\", \"staff\", \"admin\", \"other\"])\n",
        "with col2:\n",
        "    model_or_mode = st.selectbox(\"Model/mode\", [\"baseline\", \"main\", \"ablation\"])\n",
        "with col3:\n",
        "    k = st.slider(\"Top-K evidence\", min_value=1, max_value=8, value=3)\n",
        "\n",
        "query = st.text_area(\"Enter your question / task input\", height=120, placeholder=\"e.g., What time does the shuttle run?\")\n",
        "run_btn = st.button(\"Run\")\n",
        "\n",
        "if run_btn and query.strip():\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Your real pipeline output:\n",
        "    # returns dict with: answer, evidence (list), evidence_ids, latency_ms, confidence, faithfulness\n",
        "    result = run_capstone_pipeline(query, k=k)\n",
        "\n",
        "    # Prefer pipeline latency if present; otherwise use measured latency\n",
        "    latency_ms = result.get(\"latency_ms\", (time.time() - t0) * 1000)\n",
        "\n",
        "    # Artifacts = evidence IDs (required)\n",
        "    artifacts = result.get(\"evidence_ids\", [])\n",
        "\n",
        "    # Quality signal = faithfulness (or fallback)\n",
        "    quality_signal = result.get(\"faithfulness\", \"OK\")\n",
        "\n",
        "    # --- Response panel ---\n",
        "    st.subheader(\"Response\")\n",
        "    st.write(result.get(\"answer\", \"\"))\n",
        "\n",
        "    # --- Evidence panel ---\n",
        "    with st.expander(\"Evidence (IDs + preview)\", expanded=True):\n",
        "        st.write(\"Evidence IDs:\", artifacts)\n",
        "        ev = result.get(\"evidence\", [])\n",
        "        if ev:\n",
        "            st.table(pd.DataFrame(ev))\n",
        "        else:\n",
        "            st.info(\"No evidence returned.\")\n",
        "\n",
        "    # --- Metrics panel ---\n",
        "    st.subheader(\"Metrics\")\n",
        "    st.write({\n",
        "        \"latency_ms\": round(float(latency_ms), 2),\n",
        "        \"retrieval_configuration\": model_or_mode,\n",
        "        \"confidence\": result.get(\"confidence\", None),\n",
        "        \"faithfulness\": result.get(\"faithfulness\", None),\n",
        "    })\n",
        "\n",
        "    # --- Log event (required by handout) ---\n",
        "    log_event(\n",
        "        event_type=\"query\",\n",
        "        user_task_type=user_task_type,\n",
        "        model_or_mode=model_or_mode,\n",
        "        latency_ms=latency_ms,\n",
        "        artifact_ids=artifacts,\n",
        "        quality_signal=quality_signal,\n",
        "        notes=\"week4 smartcampus demo\"\n",
        "    )\n",
        "\n",
        "    st.success(f\"Logged event to {LOG_FILE}\")\n",
        "'''\n",
        "\n",
        "app_path = Path(\"./app\") / \"main.py\"\n",
        "app_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "app_path.write_text(streamlit_app, encoding=\"utf-8\")\n",
        "print(\"Wrote Streamlit app:\", app_path)\n",
        "print(\"\\nRun locally:\")\n",
        "print(\"  pip install streamlit pandas pymupdf\")\n",
        "print(\"  streamlit run app/main.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl9RgyOHiceQ"
      },
      "source": [
        "### 6.1) Generate `requirements.txt` and a starter `README.md` (Recommended)\n",
        "\n",
        "For deployment reproducibility, create a minimal dependency list and run instructions.\n",
        "If these files already exist, this cell will **not overwrite** them unless `FORCE_OVERWRITE=True`.\n"
      ],
      "id": "sl9RgyOHiceQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCuXqLsIiceQ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "FORCE_OVERWRITE = False\n",
        "\n",
        "req_path = Path('requirements.txt')\n",
        "readme_path = Path('README.md')\n",
        "\n",
        "requirements_text = \"\"\"streamlit\n",
        "pandas\n",
        "numpy\n",
        "requests\n",
        "python-dotenv\n",
        "# Add your ML/LLM packages below (examples):\n",
        "# scikit-learn\n",
        "# torch\n",
        "# transformers\n",
        "\"\"\"\n",
        "\n",
        "readme_text = f\"\"\"# {cfg.project_title or 'CS 5588 Capstone'} — Week 4 Hands-On\n",
        "\n",
        "**Team:** {cfg.team_name}\n",
        "**Stakeholder/User:** {cfg.stakeholder}\n",
        "\n",
        "## Deployment Link\n",
        "- (Paste your deployed URL here)\n",
        "\n",
        "## Run Locally\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "streamlit run app/main.py\n",
        "```\n",
        "\n",
        "## What this demo does\n",
        "- Briefly describe the product workflow (2–4 bullets).\n",
        "\n",
        "## Logs\n",
        "- Logs are written to `logs/week4_events.csv`.\n",
        "\n",
        "## Week 4 Metrics Summary\n",
        "- Impact metrics:\n",
        "- Technical metrics:\n",
        "\n",
        "## Failure & Risk\n",
        "- Link or summary of `reports/week4_failure_risk.md`.\n",
        "\"\"\"\n",
        "\n",
        "def write_if_missing(path: Path, content: str):\n",
        "    if path.exists() and not FORCE_OVERWRITE:\n",
        "        print(f\"Exists (skipping): {path}\")\n",
        "        return\n",
        "    path.write_text(content.strip() + \"\\n\", encoding='utf-8')\n",
        "    print(f\"Wrote: {path}\")\n",
        "\n",
        "write_if_missing(req_path, requirements_text)\n",
        "write_if_missing(readme_path, readme_text)\n"
      ],
      "id": "qCuXqLsIiceQ"
    },
    {
      "cell_type": "markdown",
      "id": "7f1e2d99",
      "metadata": {
        "id": "7f1e2d99"
      },
      "source": [
        "## 7) Deployment readiness plan (Required)\n",
        "In your README or `reports/`, include:\n",
        "- **Deployment target:** (HF Spaces / Streamlit Cloud / Render / Railway)\n",
        "- **Data handling:** what is included vs excluded from repo\n",
        "- **Monitoring plan:** what you log and how you review it\n",
        "- **Governance / guardrails:** what the system refuses to do, and why\n",
        "- **Architecture diagram:** a simple block diagram of components and data flow\n",
        "\n",
        "Below is a starter diagram description you can paste into your report (replace with your architecture).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0bae87f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bae87f5",
        "outputId": "7541abb9-60e2-4dcf-ec62-759b60befda6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: requirements.txt\n",
            "Wrote: README.md\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "FORCE_OVERWRITE = False\n",
        "\n",
        "req_path = Path(\"requirements.txt\")\n",
        "readme_path = Path(\"README.md\")\n",
        "\n",
        "requirements_text = (\n",
        "\"streamlit\\n\"\n",
        "\"pandas\\n\"\n",
        "\"numpy\\n\"\n",
        "\"pymupdf\\n\"\n",
        ")\n",
        "\n",
        "readme_text = (\n",
        "\"# SmartCampus — Week 4 Capstone Demo\\n\\n\"\n",
        "\"Team: HappyGroup\\n\"\n",
        "\"Stakeholder/User: UMKC students, visitors, and campus staff\\n\\n\"\n",
        "\"---\\n\\n\"\n",
        "\"Deployment Link:\\n\"\n",
        "\"(Paste your deployed URL here after deploying)\\n\\n\"\n",
        "\"---\\n\\n\"\n",
        "\"Run Locally:\\n\"\n",
        "\"1. pip install -r requirements.txt\\n\"\n",
        "\"2. streamlit run app/main.py\\n\\n\"\n",
        "\"---\\n\\n\"\n",
        "\"What This Demo Does:\\n\"\n",
        "\"- Accepts UMKC campus questions\\n\"\n",
        "\"- Retrieves evidence from uploaded PDFs\\n\"\n",
        "\"- Displays grounded answer + evidence + metrics\\n\"\n",
        "\"- Logs interactions to logs/week4_events.csv\\n\"\n",
        ")\n",
        "\n",
        "def write_if_missing(path: Path, content: str):\n",
        "    if path.exists() and not FORCE_OVERWRITE:\n",
        "        print(f\"Exists (skipping): {path}\")\n",
        "        return\n",
        "    path.write_text(content, encoding=\"utf-8\")\n",
        "    print(f\"Wrote: {path}\")\n",
        "\n",
        "write_if_missing(req_path, requirements_text)\n",
        "write_if_missing(readme_path, readme_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2850390",
      "metadata": {
        "id": "a2850390"
      },
      "source": [
        "## 8) Failure & risk analysis (Required)\n",
        "Document **one realistic deployment-level failure** and how you will detect/mitigate it.\n",
        "\n",
        "Examples:\n",
        "- Wrong evidence leads to wrong user decision\n",
        "- Data drift reduces model performance\n",
        "- Retrieval returns irrelevant context causing hallucination\n",
        "- Latency spikes make product unusable\n",
        "\n",
        "Below is a short template you can paste into your report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6995b535",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6995b535",
        "outputId": "ec1e4f14-5365-45ac-f16f-46b6ee646793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: reports/week4_failure_risk.md\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "risk_path = Path(\"./reports/week4_failure_risk.md\")\n",
        "risk_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "risk_text = \"\"\"\n",
        "# Week 4 Failure & Risk Analysis\n",
        "\n",
        "## Failure scenario (realistic)\n",
        "\n",
        "Retrieval returns irrelevant or incomplete evidence from campus PDFs,\n",
        "leading to an incorrect grounded answer.\n",
        "\n",
        "Example user input that triggers it:\n",
        "\"What time does the shuttle stop running during finals week?\"\n",
        "\n",
        "If the system retrieves the wrong semester schedule PDF or a non-shuttle document,\n",
        "the answer may be incorrect but still appear grounded.\n",
        "\n",
        "---\n",
        "\n",
        "## Impact (stakeholder/product)\n",
        "\n",
        "Wrong decision:\n",
        "A student may miss the last shuttle due to incorrect stop time.\n",
        "\n",
        "Potential harm/cost:\n",
        "- Missed transportation\n",
        "- Safety risk (late-night campus travel)\n",
        "- Loss of trust in SmartCampus system\n",
        "\n",
        "This is especially critical for safety-related or time-sensitive queries.\n",
        "\n",
        "---\n",
        "\n",
        "## Detection signals (monitoring)\n",
        "\n",
        "We detect this using log signals:\n",
        "\n",
        "- Low confidence score (< 0.65)\n",
        "- Faithfulness indicator = \"low_confidence_refusal_recommended\"\n",
        "- Citation coverage = 0\n",
        "- Spike in user re-queries for similar shuttle-related questions\n",
        "- Sudden drop in trust_verification_rate\n",
        "\n",
        "Threshold examples:\n",
        "- Low-confidence rate > 25%\n",
        "- Citation coverage < 70%\n",
        "- Latency spike > 3000 ms\n",
        "\n",
        "---\n",
        "\n",
        "## Mitigation\n",
        "\n",
        "Guardrails:\n",
        "- If confidence < threshold → return explicit refusal message\n",
        "- Always require at least one citation for time-sensitive queries\n",
        "\n",
        "Model/Data Fix:\n",
        "- Improve chunking granularity\n",
        "- Add domain filtering (e.g., only shuttle domain for shuttle queries)\n",
        "- Add reranker for higher retrieval precision\n",
        "\n",
        "Evaluation Fix:\n",
        "- Add shuttle-specific evaluation queries\n",
        "- Track Precision@K on shuttle documents\n",
        "- Add confusion matrix for domain mis-retrieval\n",
        "\n",
        "---\n",
        "\n",
        "## Post-mortem plan\n",
        "\n",
        "Next sprint changes:\n",
        "- Add domain classifier before retrieval\n",
        "- Implement hybrid retrieval tuning (alpha parameter optimization)\n",
        "- Add monitoring dashboard for confidence trends\n",
        "- Add weekly log review process\n",
        "\n",
        "Goal:\n",
        "Prevent incorrect time-sensitive decisions and increase trust rate above 90%.\n",
        "\"\"\"\n",
        "\n",
        "risk_path.write_text(risk_text, encoding=\"utf-8\")\n",
        "print(\"Wrote:\", risk_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app/main.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWmiufOLlnbR",
        "outputId": "3646e022-b2b2-4793-836b-64ffc4eb060c"
      },
      "id": "OWmiufOLlnbR",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.57.225.111:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception in callback Loop._read_from_self\n",
            "handle: <Handle Loop._read_from_self>\n",
            "Traceback (most recent call last):\n",
            "  File \"uvloop/cbhandles.pyx\", line 66, in uvloop.loop.Handle._run\n",
            "  File \"uvloop/loop.pyx\", line 399, in uvloop.loop.Loop._read_from_self\n",
            "  File \"uvloop/loop.pyx\", line 404, in uvloop.loop.Loop._invoke_signals\n",
            "  File \"uvloop/loop.pyx\", line 379, in uvloop.loop.Loop._ceval_process_signals\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/bootstrap.py\", line 43, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/server/server.py\", line 529, in stop\n",
            "    cli_util.print_to_cli(\"  Stopping...\", fg=\"blue\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/cli_util.py\", line 34, in print_to_cli\n",
            "    click.secho(message, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/termui.py\", line 690, in secho\n",
            "    return echo(message, file=file, nl=nl, err=err, color=color)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/utils.py\", line 321, in echo\n",
            "    file.write(out)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/bootstrap.py\", line 43, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/server/server.py\", line 529, in stop\n",
            "    cli_util.print_to_cli(\"  Stopping...\", fg=\"blue\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/cli_util.py\", line 34, in print_to_cli\n",
            "    click.secho(message, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/termui.py\", line 690, in secho\n",
            "    return echo(message, file=file, nl=nl, err=err, color=color)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/utils.py\", line 321, in echo\n",
            "    file.write(out)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^\n",
            "RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception in callback Loop._read_from_self\n",
            "handle: <Handle Loop._read_from_self>\n",
            "Traceback (most recent call last):\n",
            "  File \"uvloop/cbhandles.pyx\", line 66, in uvloop.loop.Handle._run\n",
            "  File \"uvloop/loop.pyx\", line 399, in uvloop.loop.Loop._read_from_self\n",
            "  File \"uvloop/loop.pyx\", line 404, in uvloop.loop.Loop._invoke_signals\n",
            "  File \"uvloop/loop.pyx\", line 379, in uvloop.loop.Loop._ceval_process_signals\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/bootstrap.py\", line 43, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/server/server.py\", line 529, in stop\n",
            "    cli_util.print_to_cli(\"  Stopping...\", fg=\"blue\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/cli_util.py\", line 34, in print_to_cli\n",
            "    click.secho(message, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/termui.py\", line 690, in secho\n",
            "    return echo(message, file=file, nl=nl, err=err, color=color)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/utils.py\", line 321, in echo\n",
            "    file.write(out)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/bootstrap.py\", line 43, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/server/server.py\", line 529, in stop\n",
            "    cli_util.print_to_cli(\"  Stopping...\", fg=\"blue\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/cli_util.py\", line 34, in print_to_cli\n",
            "    click.secho(message, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/termui.py\", line 690, in secho\n",
            "    return echo(message, file=file, nl=nl, err=err, color=color)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/utils.py\", line 321, in echo\n",
            "    file.write(out)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^\n",
            "RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b185f0",
      "metadata": {
        "id": "90b185f0"
      },
      "source": [
        "## 9) Individual reflection (Required, individual submission)\n",
        "Each student submits **one paragraph** addressing:\n",
        "\n",
        "1. What part of the capstone module is closest to production-ready?\n",
        "2. What is the biggest risk to deploying this?\n",
        "3. What would you build next sprint?\n",
        "\n",
        "Paste your paragraph in the individual survey/Canvas submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f66662",
      "metadata": {
        "id": "49f66662"
      },
      "source": [
        "## 10) Final deliverables (Team)\n",
        "Your GitHub repo should include:\n",
        "- `/app/main.py` (or equivalent app interface)\n",
        "- `/src/` (pipeline modules; reuse Week 3 work)\n",
        "- `/logs/week4_events.csv` (auto-created, with sample rows)\n",
        "- `/reports/week4_integration_brief.md`\n",
        "- `/reports/week4_architecture_notes.md`\n",
        "- `/reports/week4_failure_risk.md`\n",
        "- `requirements.txt`\n",
        "- `README.md` with:\n",
        "  - deployment link\n",
        "  - run instructions\n",
        "  - screenshots\n",
        "  - metrics summary (impact + technical)\n",
        "\n",
        "---\n",
        "\n",
        "### Tip (grading-friendly)\n",
        "Make sure a TA can:\n",
        "1) run `pip install -r requirements.txt`  \n",
        "2) run `streamlit run app/main.py`  \n",
        "3) see logs populate in `logs/week4_events.csv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e71684",
      "metadata": {
        "id": "e2e71684"
      },
      "source": [
        "## GitHub Deployment (Required Example)\n",
        "\n",
        "### Step 1 — Push your repository\n",
        "```bash\n",
        "git init\n",
        "git add .\n",
        "git commit -m \"Week4 capstone app\"\n",
        "git branch -M main\n",
        "git remote add origin https://github.com/<username>/<repo>.git\n",
        "git push -u origin main\n",
        "```\n",
        "\n",
        "### Step 2 — Deploy using GitHub-connected hosting\n",
        "Example: Streamlit Community Cloud\n",
        "\n",
        "1. Go to https://share.streamlit.io\n",
        "2. Click **New App**\n",
        "3. Select your GitHub repository\n",
        "4. Branch: `main`\n",
        "5. App path: `app/main.py`\n",
        "6. Click **Deploy**\n",
        "\n",
        "### Step 3 — Add deployment link to README\n",
        "Include the deployed URL in your repository README.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
